---
title: "Python containers and HPC"
teaching: 10
exercises: 10
questions:
objectives:
---


### MPI: requirements for containers

In order to scale a containerised application across multiple nodes in an HPC cluster, we need to be able to spawn multiple *singularity* processes through the host MPI (`mpirun`, or `srun` in the case of Slurm).  Each singularity process will launch its own application instance; the set of instances will communicate using the underlying host MPI framework.  
Singularity has been designed to support spawning and inter-communication of multiple instances.

However, there are still some requirements to make this setup work.  A specific goal here is to be able to effectively use the high-speed interconnect networking hardware in the HPC site, to maximise performance.  
Different approaches are possible, in particular the *hybrid* (or *host*) model, and the *bind* model (see also [Singularity and MPI applications](https://sylabs.io/guides/3.6/user-guide/mpi.html)).  

The *hybrid* model relies on configuring the software build in the container so that it is optimised for the host hardware; this add complexity to the Dockerfile and reduces portability.  
The *bind* model, on the other hand, shifts the complexity of configuring for the host interconnect at the runtime configuration; this improves portability, but requires some care to achieve maximum performance.

Let's outline the key requirements for the *bind* model.

* A host MPI installation must be present to spawn the MPI processes.

* An MPI installation is required in the container, to compile the application.  Also, during build the application must be linked *dynamically* to the MPI libraries, so as to have the capability of using the host ones at runtime.  Note how dynamic linking is typically the default behaviour on Linux systems.  

* The container and host MPI installations need to be *ABI* (Application Binary Interface) *compatible*. This is because the application in the container is built with the former but runs with the latter.  
At present, there are just two families of MPI implementations, not ABI compatible with each other: MPICH (with IntelMPI and MVAPICH) and OpenMPI.  

* Bind mounts and environment variables need to be setup at singularity runtime, so that the containerised MPI application can use the host MPI libraries at runtime.  If the HPC system you're using has high speed interconnect infrastructure, you will need to expose the corresponding system libraries in the container, too.  
In practice, you would need to use `SINGULARITY_BINDPATH` to mount appropriate host directory paths, and then `SINGULARITYENV_LD_LIBRARY_PATH` to let the application know where to look for the required library files.  
Overall, this can be a challenging task for a user, as it requires knowing details on the installed software stack.  System administrators typically have the required know-how to execute this.


## MPI and Singularity at Pawsey
> 
> Pawsey maintains MPICH and OpenMPI base images at [pawsey/mpich-base](https://hub.docker.com/r/pawsey/mpich-base) and [pawsey/openmpi-base](https://hub.docker.com/r/pawsey/openmpi-base), respectively.  
> At the moment, only Docker images are provided, which of course can also be used by Singularity.
> 
> All Pawsey systems have installed at least one MPICH ABI compatible implementation: CrayMPICH on the Crays (*Magnus* and *Galaxy), IntelMPI on *Zeus*, *Topaz* and *Garrawarla*.  Therefore, MPICH is the recommended MPI library to install in container images.  
> Zeus, Topaz and Garrawarla also have OpenMPI, so images built over this MPI family can run in these clusters, upon appropriate configuration of the shell environment (see below).
> 
> At the time of writing, `singularity` modules at Pawsey configure the shell environment to enable use of MPICH and the high-speed interconnects.  
> OpenMPI-enabled *singularity* modules are under development.
{: .callout}


### Python and MPI: the example of *mpi4py*

At the end of the previous episode, we discussed a Dockerfile to build an MPI enabled container shipping `mpi4py` (files under `2.mpi4py/`).  
For convenience, here's the Dockerfile again, see symlinked `Dockerfile.1-mpi4py`:

```
FROM python:3.8-slim

RUN apt-get update -qq \
      && apt-get -y --no-install-recommends install \
         build-essential \
         ca-certificates \
         gdb \
         gfortran \
         wget \
      && apt-get clean all \
      && rm -r /var/lib/apt/lists/*

ARG MPICH_VERSION="3.1.4"
ARG MPICH_CONFIGURE_OPTIONS="--enable-fast=all,O3 --prefix=/usr"
ARG MPICH_MAKE_OPTIONS="-j4"

RUN mkdir -p /tmp/mpich-build \
      && cd /tmp/mpich-build \
      && wget http://www.mpich.org/static/downloads/${MPICH_VERSION}/mpich-${MPICH_VERSION}.tar.gz \
      && tar xvzf mpich-${MPICH_VERSION}.tar.gz \
      && cd mpich-${MPICH_VERSION}  \
      && ./configure ${MPICH_CONFIGURE_OPTIONS} \
      && make ${MPICH_MAKE_OPTIONS} \
      && make install \
      && ldconfig \
      && cp -p /tmp/mpich-build/mpich-${MPICH_VERSION}/examples/cpi /usr/bin/ \
      && cd / \
      && rm -rf /tmp/mpich-build

ARG MPI4PY_VERSION="3.0.3"

RUN pip --no-cache-dir install --no-deps mpi4py==${MPI4PY_VERSION}

CMD [ "/bin/bash" ]
```
{: .source}

Now you see that, as MPICH was used to build `mpi4py` in the container, then at runtime you'll need to use a host MPI library which is ABI compatible with MPICH.

Remember we built the container image `m1` out of this Dockerfile; let's play with it on one of Pawsey's HPC clusters, *Zeus* in this instance.  
First, we need to convert the image to the singularity format:

```
$ singularity pull docker-daemon:m1:latest
```
{: .bash}

And then to transfer the resulting file, `m1_latest.sif`, onto the cluster.  
Here we'll also assume that we've got a copy of this github repo on Zeus, and that we've executed `module load singularity`.

Let's convince ourselves that the singularity module does indeed take care of the host MPI/interconnect configuration; here the *Intel MPI* installation is used.  Look for `SINGULARITY_BINDPATH` and `SINGULARITYENV_LD_LIBRARY_PATH` in this output:

```
$ module show singularity
```
{: .bash}

```
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   /pawsey/sles12sp3/modulefiles/devel/singularity/3.5.3.lua:
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
help([[Sets up the paths you need to use singularity version 3.5.3]])
whatis("Singularity enables users to have full control of their environment. Singularity
containers can be used to package entire scientific workflows, software and
libraries, and even data.

For further information see http://singularity.lbl.gov")
whatis("Compiled with gcc/4.8.5")
load("go")
setenv("MAALI_SINGULARITY_HOME","/pawsey/sles12sp3/devel/gcc/4.8.5/singularity/3.5.3")
prepend_path("PATH","/pawsey/sles12sp3/devel/gcc/4.8.5/singularity/3.5.3/bin")
setenv("SINGULARITYENV_LD_LIBRARY_PATH","/usr/lib64:/pawsey/intel/17.0.5/compilers_and_libraries/linux/mpi/intel64/lib")
setenv("SINGULARITY_BINDPATH","/astro,/group,/scratch,/pawsey,/etc/dat.conf,/etc/libibverbs.d,/usr/lib64/libdaplofa.so.2,/usr/lib64/libdaplofa.so.2.0.0,/usr/lib64/libdat2.so.2,/usr/lib64/libdat2.so.2.0.0,/usr/lib64/libibverbs,/usr/lib64/libibverbs.so,/usr/lib64/libibverbs.so.1,/usr/lib64/libibverbs.so.1.1.14,/usr/lib64/libmlx5.so,/usr/lib64/libmlx5.so.1,/usr/lib64/libmlx5.so.1.1.14,/usr/lib64/libnl-3.so.200,/usr/lib64/libnl-3.so.200.18.0,/usr/lib64/libnl-cli-3.so.200,/usr/lib64/libnl-cli-3.so.200.18.0,/usr/lib64/libnl-genl-3.so.200,/usr/lib64/libnl-genl-3.so.200.18.0,/usr/lib64/libnl-idiag-3.so.200,/usr/lib64/libnl-idiag-3.so.200.18.0,/usr/lib64/libnl-nf-3.so.200,/usr/lib64/libnl-nf-3.so.200.18.0,/usr/lib64/libnl-route-3.so.200,/usr/lib64/libnl-route-3.so.200.18.0,/usr/lib64/librdmacm.so,/usr/lib64/librdmacm.so.1,/usr/lib64/librdmacm.so.1.0.14")
setenv("SINGULARITY_CACHEDIR","/group/pawsey0001/mdelapierre/.singularity")
```
{: .output}

As the first thing, we want to check how `mpi4py` links to available MPI libraries.  
Here we're looking for the right path location:

```
$ singularity exec m1_latest.sif find /usr -name "mpi4py*"
```
{: .bash}

```
/usr/local/lib/python3.8/site-packages/mpi4py
/usr/local/lib/python3.8/site-packages/mpi4py/include/mpi4py
/usr/local/lib/python3.8/site-packages/mpi4py/include/mpi4py/mpi4py.MPI.h
/usr/local/lib/python3.8/site-packages/mpi4py/include/mpi4py/mpi4py.MPI_api.h
/usr/local/lib/python3.8/site-packages/mpi4py/include/mpi4py/mpi4py.h
/usr/local/lib/python3.8/site-packages/mpi4py/include/mpi4py/mpi4py.i
/usr/local/lib/python3.8/site-packages/mpi4py-3.0.3.dist-info
```
{: .output}

```
$ singularity exec m1_latest.sif ls /usr/local/lib/python3.8/site-packages/mpi4py
```
{: .bash}

```
MPI.cpython-38-x86_64-linux-gnu.so  __init__.pxd  __main__.py  bench.py				  futures  lib-pmpi    mpi.cfg
MPI.pxd				    __init__.py   __pycache__  dl.cpython-38-x86_64-linux-gnu.so  include  libmpi.pxd  run.py
```
{: .output}

And now we're using the Linux utility `ldd` to get information on libraries that the `mpi4py` MPI library links to:

```
$ singularity exec m1_latest.sif ldd /usr/local/lib/python3.8/site-packages/mpi4py/MPI.cpython-38-x86_64-linux-gnu.so
```
{: .bash}

```
	linux-vdso.so.1 (0x00007ffc511fe000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f3c4e1c1000)
	libmpi.so.12 => /pawsey/intel/17.0.5/compilers_and_libraries/linux/mpi/intel64/lib/libmpi.so.12 (0x00007f3c4d48b000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f3c4d46a000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f3c4d2a9000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f3c4e348000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f3c4d29f000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f3c4d283000)
```
{: .output}

Indeed, you can see that `mpi4py` binds to the host Intel MPI library: `/pawsey/intel/17.0.5/compilers_and_libraries/linux/mpi/intel64/lib/libmpi.so.12`.

Now, let's unset `LD_LIBRARY_PATH` in the container:

```
$ singularity exec m1_latest.sif bash -c 'unset LD_LIBRARY_PATH ; ldd /usr/local/lib/python3.8/site-packages/mpi4py/MPI.cpython-38-x86_64-linux-gnu.so'
```
{: .bash}

```
	linux-vdso.so.1 (0x00007fff5cfb9000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f3d0ed8a000)
	libmpi.so.12 => /usr/lib/libmpi.so.12 (0x00007f3d0eb0d000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f3d0eaec000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f3d0e92b000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f3d0ef11000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f3d0e921000)
	libgfortran.so.5 => /usr/lib/x86_64-linux-gnu/libgfortran.so.5 (0x00007f3d0e6b3000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f3d0e52e000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f3d0e514000)
	libquadmath.so.0 => /usr/lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007f3d0e4d2000)
	libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f3d0e2b4000)
```
{: .output}

Now we're falling back to the container MPI, in `/usr/lib/libmpi.so.12`.  You'd never use this in production, but it's good to have a close look for once.

All right, now let's get an interactive Slurm allocation, with two cores on two distinct nodes:

```
$ salloc -n 2 --ntasks-per-node=1 -p debugq -t 10:00
```
{: .bash}


And then see `mpi4py` in action, starting with the simple `hello-mpi4py.py` script:

```
$ srun singularity exec m1_latest.sif python3 hello-mpi4py.py
```
{: .bash}

```
Hello World! I am process 1 of 2 on z127.
Hello World! I am process 0 of 2 on z126.
```
{: .output}

Sweet!

Can we have a little look at performance?  
The provided script `osu_bw.py` implements a small core to core bandwidth test:

```
$ srun singularity exec m1_latest.sif python3 osu_bw.py 
```
{: .bash}

```
# MPI Bandwidth Test
# Size [B]    Bandwidth [MB/s]
1                         0.57
2                         1.19
4                         2.53
8                         4.37
16                        6.24
32                       12.35
64                       24.63
128                      48.17
256                     106.87
512                     204.93
1024                    395.58
2048                    754.55
4096                   1391.42
8192                   2467.49
16384                  2583.36
32768                  2752.52
65536                  2842.06
131072                 2910.04
262144                 4469.70
524288                 6872.53
1048576                8775.95
```
{: .output}

With a 1 MB message, we get about 8 GB/s of bandwidth.

What happens if we bypass the interconnect by again unsetting `LD_LIBRARY_PATH`?

```
$ srun singularity exec m1_latest.sif bash -c 'unset LD_LIBRARY_PATH ; python3 osu_bw.py'
```
{: .bash}

```
# MPI Bandwidth Test
# Size [B]    Bandwidth [MB/s]
1                         0.26
2                         0.54
4                         1.06
8                         2.03
16                        3.09
32                        5.92
64                       11.86
128                      17.68
256                      34.19
512                      54.21
1024                     74.66
2048                     92.61
4096                    104.83
8192                    111.23
16384                   113.94
32768                   116.07
65536                   116.58
131072                  116.77
262144                  117.19
524288                  117.38
1048576                 117.45
```
{: .output}

Bandwidth has dropped to only 100 MB/s for a 1 MB message!  Again, you'd never do this in production, but this is hopefully a good demonstration of why a proper MPI/interconnect setup matters.

Remember to `exit` your interactive Slurm session when you're done.


### One more MPI example: parallel *h5py*

Let's see how we can build a container image with the `h5py` library, used to handle large datasets using the HDF5 file format.  We're going to build on top of what we learn on `mpi4py`, and get enable support for the package.

Here is the Dockerfile, see `Dockerfile.2-h5py`:

```
FROM python:3.8-slim

RUN apt-get update -qq \
      && apt-get -y --no-install-recommends install \
         build-essential \
         ca-certificates \
         gdb \
         gfortran \
         wget \
      && apt-get clean all \
      && rm -r /var/lib/apt/lists/*

ARG MPICH_VERSION="3.1.4"
ARG MPICH_CONFIGURE_OPTIONS="--enable-fast=all,O3 --prefix=/usr"
ARG MPICH_MAKE_OPTIONS="-j4"

RUN mkdir -p /tmp/mpich-build \
      && cd /tmp/mpich-build \
      && wget http://www.mpich.org/static/downloads/${MPICH_VERSION}/mpich-${MPICH_VERSION}.tar.gz \
      && tar xvzf mpich-${MPICH_VERSION}.tar.gz \
      && cd mpich-${MPICH_VERSION}  \
      && ./configure ${MPICH_CONFIGURE_OPTIONS} \
      && make ${MPICH_MAKE_OPTIONS} \
      && make install \
      && ldconfig \
      && cp -p /tmp/mpich-build/mpich-${MPICH_VERSION}/examples/cpi /usr/bin/ \
      && cd / \
      && rm -rf /tmp/mpich-build

ARG MPI4PY_VERSION="3.0.3"

RUN pip --no-cache-dir install --no-deps mpi4py==${MPI4PY_VERSION}

# Install HDF5-parallel

ARG HDF5_VERSION="1.10.4"
ARG HDF5_CONFIGURE_OPTIONS="--prefix=/usr --enable-parallel CC=mpicc"
ARG HDF5_MAKE_OPTIONS="-j4"

RUN mkdir -p /tmp/hdf5-build \
      && cd /tmp/hdf5-build \
      && HDF5_VER_MM="${HDF5_VERSION%.*}" \
      && wget https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-${HDF5_VER_MM}/hdf5-${HDF5_VERSION}/src/hdf5-${HDF5_VERSION}.tar.gz \
      && tar xzf hdf5-${HDF5_VERSION}.tar.gz \
      && cd hdf5-${HDF5_VERSION} \
      && ./configure ${HDF5_CONFIGURE_OPTIONS} \
      && make ${HDF5_MAKE_OPTIONS} \
      && make install \
      && ldconfig \
      && cd / \
      && rm -rf /tmp/hdf5-build

ARG H5PY_VERSION="2.10.0"
RUN CC="mpicc" HDF5_MPI="ON" HDF5_DIR="/usr" pip --no-cache-dir install --no-deps --no-binary=h5py h5py==${H5PY_VERSION}

CMD [ "/bin/bash" ]
```
{: .source}

By default, `pip` would install the serial version of `h5py`, so we need to build both `HDF5` and `h5py` from source.
For `HDF5`, see how we're using the MPI C compiler, `mpicc`:

```
ARG HDF5_CONFIGURE_OPTIONS="--prefix=/usr --enable-parallel CC=mpicc"
```
{: .source}

And then for `h5py` see how we provide MPI specific information:

```
RUN CC="mpicc" HDF5_MPI="ON" HDF5_DIR="/usr" pip --no-cache-dir install --no-deps --no-binary=h5py h5py==${H5PY_VERSION}
```
{: .source}

Now, back to our build workstation, let us build the container:

```
$ docker build -t m2 -f Dockerfile.2-h5py .
```
{: .bash}

And convert it to singularity format:

```
$ singularity pull docker-daemon:m2:latest
```
{: .bash}

We can again use `ldd` to inspect library linking:

```
$ singularity exec m2_latest.sif find /usr -name "h5py*"
```
{: .bash}

```
/usr/local/lib/python3.8/site-packages/h5py
/usr/local/lib/python3.8/site-packages/h5py/__pycache__/h5py_warnings.cpython-38.pyc
/usr/local/lib/python3.8/site-packages/h5py/h5py_warnings.py
/usr/local/lib/python3.8/site-packages/h5py-2.10.0-py3.8.egg-info
```
{: .output}


```
$ singularity exec m2_latest.sif ls /usr/local/lib/python3.8/site-packages/h5py
```
{: .bash}

```
__init__.py				 h5.cpython-38-x86_64-linux-gnu.so    h5i.cpython-38-x86_64-linux-gnu.so   h5t.cpython-38-x86_64-linux-gnu.so
__pycache__				 h5a.cpython-38-x86_64-linux-gnu.so   h5l.cpython-38-x86_64-linux-gnu.so   h5z.cpython-38-x86_64-linux-gnu.so
_conv.cpython-38-x86_64-linux-gnu.so	 h5ac.cpython-38-x86_64-linux-gnu.so  h5o.cpython-38-x86_64-linux-gnu.so   highlevel.py
_errors.cpython-38-x86_64-linux-gnu.so	 h5d.cpython-38-x86_64-linux-gnu.so   h5p.cpython-38-x86_64-linux-gnu.so   ipy_completer.py
_hl					 h5ds.cpython-38-x86_64-linux-gnu.so  h5pl.cpython-38-x86_64-linux-gnu.so  tests
_objects.cpython-38-x86_64-linux-gnu.so  h5f.cpython-38-x86_64-linux-gnu.so   h5py_warnings.py			   utils.cpython-38-x86_64-linux-gnu.so
_proxy.cpython-38-x86_64-linux-gnu.so	 h5fd.cpython-38-x86_64-linux-gnu.so  h5r.cpython-38-x86_64-linux-gnu.so   version.py
defs.cpython-38-x86_64-linux-gnu.so	 h5g.cpython-38-x86_64-linux-gnu.so   h5s.cpython-38-x86_64-linux-gnu.so
```
{: .output}

```
$ singularity exec m2_latest.sif ldd /usr/local/lib/python3.8/site-packages/h5py/h5.cpython-38-x86_64-linux-gnu.so
```
{: .bash}

```
	linux-vdso.so.1 (0x00007ffee5f27000)
	libhdf5.so.103 => /usr/lib/libhdf5.so.103 (0x00007f4e68e19000)
	libhdf5_hl.so.100 => /usr/lib/libhdf5_hl.so.100 (0x00007f4e68df3000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f4e68dce000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f4e68c0d000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f4e68c08000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f4e68a85000)
	libmpi.so.12 => /usr/lib/libmpi.so.12 (0x00007f4e68806000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f4e69228000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f4e687fc000)
	libgfortran.so.5 => /usr/lib/x86_64-linux-gnu/libgfortran.so.5 (0x00007f4e6858e000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f4e68574000)
	libquadmath.so.0 => /usr/lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007f4e68532000)
	libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f4e68312000)
```
{: .output}

On our build cluster, without any host MPI binding, `h5p` libraries bind to the container MPICH installation, `/usr/lib/libmpi.so.12`.

But, if we transfer the image file to Zeus and run the command there ..

```
	linux-vdso.so.1 (0x00007ffe76ba4000)
	libhdf5.so.103 => /usr/lib/libhdf5.so.103 (0x00007f03e2509000)
	libhdf5_hl.so.100 => /usr/lib/libhdf5_hl.so.100 (0x00007f03e24e3000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f03e24be000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f03e22fd000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f03e22f8000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f03e2175000)
	libmpi.so.12 => /pawsey/intel/17.0.5/compilers_and_libraries/linux/mpi/intel64/lib/libmpi.so.12 (0x00007f03e143d000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f03e2918000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f03e1433000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f03e1419000)
```
{: .output}

.. we get linking to the host Intel MPI, `/pawsey/intel/17.0.5/compilers_and_libraries/linux/mpi/intel64/lib/libmpi.so.12`!






```

```
{: .bash}

```

```
{: .output}


### Intel Python




### Pawsey Python base images





