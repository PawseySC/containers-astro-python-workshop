---
title: "Python containers: build best practices"
teaching: 10
exercises: 10
questions:
objectives:
---


> ## Little trick: get rid of *sudo docker*
> 
> Docker requires administrative rights to be used, so in principle every command requires *sudo*, as in `sudo docker`.  
> 
> To save typing, you may want to add your user to the `docker` user group:
> 
> ```
> sudo usermod -aG docker $USER
> ```
> {: .bash}
> 
> then exit the terminal session and open a fresh new one.  
> 
> From now on, you can run `docker` without *sudo*.  
> Note that under the hood *docker* commands will still require admin rights.
> {: .callout}


### Useful base images

Depending on the type of application you need to containerise, various public images can represent an effective starting point, in providing ready-to-use utilities that have been containerised, tested and optimised by their own authors.  For instance:

* Python images, such as `python:3.8` and `python:3.8-slim` (a lightweight version);
* Conda images by [Anaconda](https://www.anaconda.com), such as `continuumio/miniconda3:4.8.2`;
* Intel optimised Python images, *i.e.* `intelpython/intelpython3_core:2020.2` and `intelpython/intelpython3_full:2020.2`;
* Pawsey MPI images, such as `pawsey/mpich-base:3.1.4_ubuntu18.04` and `pawsey/openmpi-base:4.0.2_ubuntu18.04`;
* CUDA images, including:
    - `nvidia/cuda:10.2-base-ubuntu18.04`;
    - `nvidia/cuda:10.2-runtime-ubuntu18.04`;
    - `nvidia/cuda:10.2-devel-ubuntu18.04`;
* Jupyter images, in particular the `jupyter/` repository by [Jupyter Docker Stacks](https://jupyter-docker-stacks.readthedocs.io) (unfortunately making extensive use of the `latest` tag), for instance:
    - Base Jupyter: `jupyter/base-notebook:latest`;
    - Jupyter with scientific Python packages: `jupyter/scipy-notebook:latest`;
* OS images, such as `ubuntu:18.04`, `debian:buster` and `centos:7`.

Note that the `python`, `miniconda3`, and `intelpython` images are all based on Debian OS (currently version 10 *Buster*).  The Pawsey `mpich-base` image is based on Ubuntu 18.04.

All of the mentioned images are currently hosted in Docker Hub.  In addition to the image itself, it is worth having an idea of how the corresponding Dockerfile looks like, both to know how the image was created and to get tips on how to optimally use it.  
Having the Dockerfiles is also useful in case one needs an image with multiple utilities.  Then, intelligently merging the Dockerfiles will do the job.

In this workshop we're going to use the following:

* Python: `python:3.8-slim`, the `slim` version being preferred not to include in the container unneeded packages.  [Image](https://hub.docker.com/_/python).  [Dockerfile](https://github.com/docker-library/python/blob/master/3.8/buster/slim/Dockerfile);
* Conda: `continuumio/miniconda3:4.8.2`, with `miniconda3` again preferred over `anaconda3` to exclude unneeded packages.  [Image](https://hub.docker.com/r/continuumio/miniconda3).  [Dockerfile](https://github.com/ContinuumIO/docker-images/blob/master/miniconda3/debian/Dockerfile);
* Intel Python: `intelpython/intelpython3_core:2020.2`, with `_core` preferred over `_full`.  [Image](https://hub.docker.com/r/intelpython/intelpython3_core).  [Dockerfile](https://github.com/IntelPython/container-images/blob/master/configs/intelpython3_core/Dockerfile);
* Pawsey MPICH: `pawsey/mpich-base:3.1.4_ubuntu18.04`.  [Image](https://hub.docker.com/repository/docker/pawsey/mpich-base).  [Dockerfile](https://github.com/PawseySC/pawsey-containers/blob/master/mpich-base/Dockerfile).


### Some building best practices

Always keep in mind that writing a Dockerfile is almost an art, which you can refine over time with practice.  Here we don't mean to be exhaustive; instead, we're providing some good practices to start with, with some *Ubuntu/Debian* examples where relevant.  These practices will then be applied to the examples further down, and in the next episodes.


1. **Condensate commands into few `RUN` instructions**

    This reduces the number of image caching *layers*, and thus the total size of the image.  
    
    Why is it so?  
    At one extreme, imagine one `RUN` per command.  Any edit to the same files in the container or any deletion of unnecessary files would create a separate layer (*i.e.* snapshot).  The final container would be larger.  
    On the other extreme, concentrating every command in one `RUN` would minimise image size, as there would be one single layer.  As a disadvantage, readability of the Dockerfile would be reduced, and caching during build would become less effective.
    
    In the end, the best practice is to group together commands that relate to the same component of the installation.  Readability and caching are improved in this way, and you would not gain space anyway if you further grouped them, as these groups of commands operate on different files.


2. **Clean the installation process**

    Installation files that are not required by the application runtime can be deleted, contributing to reduce the final image size (when coupled with practice `1.` above).  
    
    As a first *Ubuntu* example, here's how to run a clean `apt` installation:

    In our example with samtools, you would clean the `apt` installations with the following commands:
    
    ```
    RUN apt-get update \
          && apt-get -y install \
            build-essential \
            gfortran \
          && apt-get clean all && \
          apt-get purge && \
          rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*
    ```
    {: .source}
    
    and here is how to tidy up a software compilation:
    
    ```
    RUN mkdir -p /tmp/tool-build \
          && cd /tmp/tool-build \
          [..]
          <INSTALL YOUR TOOL>
          [..]
          && cd / \
          && rm -rf /tmp/tool-build
    ```
    {: .source}


3. **Abstract package versions, if you can**

    The `ARG` instruction can be used to define variables in the Dockerfile, that only exist within the build process.  This can be especially useful to specify package versions in a more general and flexible way.
    
    As a general example:
    
    ```
    ARG TOOL_VERSION="2.0"
    ```
    {: .source}
    
    Then, you can use `$TOOL_VERSION` along the package build commands. 
    
    A nice feature of `docker build` is that you can dynamically assign these `ARG` at build time using the `--build-arg` flag, *e.g.* if you want to install version `3.0`:
    
    ```
    $ sudo docker build -t tool --build-arg TOOL_VERSION="3.0" .
    ```
    {: .bash}


4. **Consider build reproducibility, if you can**

    A big part of using containers is *runtime* reproducibility.  
    
    However, it can be worth reflecting on *build time* reproducibility, too.  
    In other words, if I re-run an image build with the same Dockerfile over time, am I getting the same image as output?  This can be important for an image curator, to be able to guarantee *to some extent* that the image they're build and shipping is the one they mean to.
    
    A couple of general advice here:
    * rely on explicit versioning of the key packages in your image as much as possible.  This may include OS version, key library and key dependency versions, end-user application version (this latter is possibly obvious); typically this involves a trade-off between versioning too much and too little;
    * for Python/Conda, track versioned packages to install using a `requirements` approach; see examples further down;
    * avoid generic package manager update commands, which make you lose control on versions.  In particular, you should avoid `apt-get upgrade`, `conda update` and so on;
    * avoid downloading sources/binaries for `latest` versions, specify the version number instead.


5. **Know and set some useful environment variables**

    Dockerfile installations are non-interactive by nature, *i.e.* no installer can ask you questions during the process.  
    In *Ubuntu/Debian*, you can define a variable prior to running any `apt` command, that informs a shell in Ubuntu or Debian that you are not able to interact, so that no questions will be asked:
    
    ```
    ENV DEBIAN_FRONTEND="noninteractive"
    ```
    {: .source} 
    
    Another pair of useful variables, again to be put at the beginning of the Dockerfile, are:
    
    ```
    ENV LANG="C.UTF-8" LC_ALL="C.UTF-8"
    ```
    {: .source}
    
    These variables are used to specify the language localisation, or *locale*, to the value *C.UTF-8* in this case.  Leaving this undefined can result, for some programs, in warnings or even in unintended behaviours (both at build and run time).


6. **Document your Dockerfile with labels**

    This is a way to provide information to third party users of your container image, to enable a more effective use.  
    For instance you can add a *maintainer* label:
    
    ```
    LABEL maintainer="john.doe@nowhere.com"
    ```
    {: .source}


7. **Think about the default command**

    The Docker instruction `CMD` can be used to set the default command that gets executed by `docker run <IMAGE>` (without arguments) or `singularity run <IMAGE>`.  
    If you don't specify it, the default will be the `CMD` specified by the base image, or the shell if the latter was not defined.  

    Our suggestion for the vast majority of containers is to set it to `/bin/bash`.  
    You might think of using an application binary instead, but in the end this is not very useful.  In fact, the `CMD` default gets overridden by any command/argument you add to the `run` commands, in which case you would need to explicitly state the main command anyway.  Moreover, if your container ships multiple executables, some will be left out anyway.

    There are only a few exception to this general advice.  
    For Python and R containers, you might set it to the `python` and `R` interpreter, respectively.  
    Do not set `CMD` for `rocker/` RStudio and `jupyter/` Jupyter images; these come with articulated setups that allow to spawn the web servers, so your choice would be ignored anyway.


### Python application



### Conda application



### MPI Python application



